{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/xiaopanzhang/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import re\n",
    "import gc\n",
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open file: dataset/training.1600000.processed.noemoticon.csv\n",
      "Dataset size: 1600000\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"dataset/training.1600000.processed.noemoticon.csv\"\n",
    "print(\"Open file:\", dataset_path)\n",
    "DATASET_COLUMNS = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "df = pd.read_csv(dataset_path, encoding=\"ISO-8859-1\" , names=DATASET_COLUMNS)\n",
    "print(\"Dataset size:\", len(df))\n",
    "\n",
    "TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
    "stop_words = stopwords.words(\"english\")\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def preprocess(text, stem=False):\n",
    "    # Remove link,user and special characters\n",
    "    text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\n",
    "    #Convert www.* or https?://* to URL\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',text)\n",
    "    \n",
    "    #Convert @username to __USERHANDLE\n",
    "    text = re.sub('@[^\\s]+','__USERHANDLE',text)  \n",
    "    \n",
    "    #Replace #word with word\n",
    "    text = re.sub(r'#([^\\s]+)', r'\\1', text)\n",
    "    \n",
    "    #trim\n",
    "    text = text.strip('\\'\"')\n",
    "    \n",
    "    # Repeating words like hellloooo\n",
    "    repeat_char = re.compile(r\"(.)\\1{1,}\", re.IGNORECASE)\n",
    "    text = repeat_char.sub(r\"\\1\\1\", text)\n",
    "    \n",
    "    tokens = []\n",
    "    for token in text.split():\n",
    "        if token not in stop_words:\n",
    "            if stem:\n",
    "                tokens.append(stemmer.stem(token))\n",
    "            else:\n",
    "                tokens.append(token)\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = 2000\n",
    "num_iter = df.shape[0] // train_size // 2\n",
    "\n",
    "gnb = MultinomialNB()\n",
    "\n",
    "df.text = df.text.apply(lambda x: preprocess(x))\n",
    "X = df[\"text\"]\n",
    "y = df[\"target\"]\n",
    "\n",
    "X, y = shuffle(X, y)\n",
    "\n",
    "X = X.tolist() \n",
    "y = y.tolist() \n",
    "\n",
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000 2799 0.69975\n",
      "4000 2880 0.72\n",
      "4000 2820 0.705\n",
      "4000 2787 0.69675\n",
      "4000 2781 0.69525\n",
      "4000 2813 0.70325\n",
      "4000 2846 0.7115\n",
      "4000 2842 0.7105\n",
      "4000 2806 0.7015\n",
      "4000 2794 0.6985\n",
      "4000 2789 0.69725\n",
      "4000 2798 0.6995\n",
      "4000 2811 0.70275\n",
      "4000 2838 0.7095\n",
      "4000 2830 0.7075\n",
      "4000 2801 0.70025\n",
      "4000 2771 0.69275\n",
      "4000 2834 0.7085\n",
      "4000 2795 0.69875\n",
      "4000 2801 0.70025\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=5, max_df=0.95, sublinear_tf = True,use_idf = True,ngram_range=(1, 2))\n",
    "X = vectorizer.fit_transform(X)\n",
    "full_vocab = vectorizer.vocabulary_\n",
    "# vectorizer_test = TfidfVectorizer(vocabulary=full_vocab)\n",
    "\n",
    "acc_list = []\n",
    "\n",
    "for i in range(num_iter - 1):\n",
    "\n",
    "    train_starting_index = i * train_size\n",
    "    train_end_index = (i + 1) * train_size\n",
    "\n",
    "    X_train = scipy.sparse.vstack((X[train_starting_index : train_end_index], X[800000 + train_starting_index : 800000 + train_end_index]))\n",
    "    \n",
    "    X_train = X_train.toarray()\n",
    "    y_train = y[train_starting_index : train_end_index] + y[800000 + train_starting_index : 800000 + train_end_index]\n",
    "\n",
    "    X_test = scipy.sparse.vstack((X[train_starting_index + train_size : train_end_index + train_size], X[800000 + train_starting_index + train_size : 800000 + train_end_index + train_size]))\n",
    "    X_test = X_test.toarray()\n",
    "    y_test = y[train_starting_index + train_size : train_end_index + train_size] + y[800000 + train_starting_index + train_size : 800000 + train_end_index + train_size]\n",
    "\n",
    "    if i == 0:\n",
    "        y_pred = gnb.partial_fit(X_train, y_train, [0, 4]).predict(X_test)\n",
    "    else:\n",
    "        y_pred = gnb.partial_fit(X_train, y_train).predict(X_test)\n",
    "    \n",
    "    y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "    correct_num = (y_test == y_pred).sum()\n",
    "    acc = correct_num / (train_size * 2) \n",
    "    print((train_size * 2), correct_num, acc)\n",
    "\n",
    "    acc_list.append(acc)\n",
    "\n",
    "    with open('acc_list.pkl', 'wb') as handle:\n",
    "        pickle.dump(acc_list, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    plt.plot(acc_list)\n",
    "    plt.xlabel('iteration')\n",
    "    plt.ylabel('accuracy (%)')\n",
    "    plt.savefig('accuracy.png')\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
